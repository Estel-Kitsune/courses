---
layout: default
title:  'Exercise: De Novo Validation'
---

##Exercise: De Novo Validation

In this exercise we will perform assembly validation of 5 assemblies. The excercise is based on real data freely available here:  
- *Staphylococcus aureus*, [GAGE dataset](http://gage.cbcb.umd.edu/data/Staphylococcus_aureus/) 

We will use 5 assemblies generated by a group of de novo assembly experts (e.g., *Salzberg S.L.*, *Philippy A.*). Have a look at http://gage.cbcb.umd.edu/. We will evaluete their assemblies using several techniques and tools and, if you want, you can add your de novo assembly and judge and compare your own results.

In order to facilitate the tutorial all steps have been precomputed and can be found here:

```
$ /proj/g2015027/assemblyValidation/AV_Exercise_executed
```

We will go through all the steps using the Velvet assembly. All assemblies have been already computed, or can be easily computed using a shell script present in each working directory. However you are free to rerun them completely (if time allows it) or you can run the various steps on your own assemblies.

### Setting up your project environment  

Copy the data from the project directory to your home folder in order to run your excersices locally. It is important that you copy the files exactly in this way as many commands that will follow rely on finding things in a precise place. **DO NOT START TO RUN SCRIPTS ALREADY: WE NEED TO FIX THE WORKING ENVIRONMENT FIRST.**

```
$ cd ~
$ rsync -r -v --progress /proj/g2015027/assemblyValidation/AV_Exercise/ AV_Exercise 
$ cd AV_Exercise/ 
$ ls 
00_tools 01_data 02_assemblies 03_eval 04_align 05_QAtools 06_FRCbam 07_REAPR 08_CEGMA
```

This will be our working directory.

In `/proj/g2015027/assemblyValidation/AV_Exercise_executed/` you can find an identical directory structure with all the exercise already computed. Use this if you want to have a look at the results.
If you soft link this folder into your directory pay attention to not delete the contens (do ***_not_*** execute `rm -r AV_Exercise_executed/`).

 The AV_Exercise folder in your home contains the following elements:

- **00_tools**: set of useful tools and commands to facilitate your work.
- **01_data**: data, Paired End reads in the folder PE (in standard format) and Mate Pair reads in the folder MP_rc (they have been reverse complemented as software we will use expect all pairs to be innies -><-)
- **02_assemblies**: de novo assemblies for *ABySS*, *Allpaths-LG*, *MaSuRCA* (a.k.a. MSR-CA), *SOAPdenovo*, *Velvet*. **Note**: In case you want to run the exercise on your own de novo assembly you must first of all rename all the contigs (Reapr wants all contigs in a form similar to ">contig_NUM"). Use the script `~/AV_Exercise/00_tools/rename_assembly_contigs.pl --assembly ORIGINAL_SEQ > RENAMED_SEQ`
- **03_eval**: first evaluation step, i.e., standard assembly statistics.
- **04_align**: reads alignments (PE and MP) against the assemblies, this is a required step for further analysis. Command `run_alignments.sh` allows you to generate all the alignments. This is time consuming (it takes 34 minutes), so you are encouraged to use the comand `run_soflinkAlignments.sh` that creates softlinks to pre-computed alignments and runs "only" PicardTools (it takes less than 2 minutes).
- **05_QA_tools**: In this folder we will execute and play around with QA-tools, a set of useful tools to asses assembly complexity/quality. The command runQA_tools.sh executes the command.
- **06_FRCbam**: In this folder we will execute and play around with FRCbam a tool to rank de novo assemblies obtained with different assemblers.
- **07_REAPR**: In this folder we will execute and play around with Reapr a tool to rank and correct assemblies. As this tool is slower than the others (37 minutes to evaluete all 5 assemblies) you will find the results already computed (some partial large files have been deleted to avoid the transfer of large files).
- **07_CEGMA**: results from CEGMA. THis tool is extremely slow and unstable so we will only look at the results

For each assembler contig (*ctg*) and scaffold (*scf*) sequences are present. We will use _only_ scaffolds but everything we say can be done on contigs too.

### Setting up the working enviorment

Now all the data you need is in your home folder, in the `AV_Exercise` directory. You still need a lot of tools though.

Setting up the working the environment (i.e., be sure that all the needed tools are available on the command line) is the first and probably most important step. 

Copy and paste the following commands (preferably one line at a time):

```
module load bioinfo-tools
module load bwa/0.7.5a
module load samtools
module load bamtools
module load picard/1.92
FRCbam=/proj/g2015027/assemblyValidation/tools/FRCbam/src/
QAtools=/proj/g2015027/assemblyValidation/tools/qaTools/
REAPR=/proj/g2015027/assemblyValidation/tools/Reapr_1.0.16/
export PATH=$PATH:$FRCbam:$QAtools:$REAPR
export PERL5LIB=/proj/g2015027/assemblyValidation/tools/File-Copy-Link-0.112/lib/
module load BioPerl/1.6.1
```

This should set up all the variables and paths that allow you to run all the needed programs. Try this:

```
$ FRC --help
```

If an help message is produced everything should be working, if it's not, feel free to panic! Another solution is to call one of us and we'll sort it out.

At this point everything should be set and you can start to work. I suggest you to start looking at folders `01_data` and `02_assemblies` to see how they are structured.

### Standard assembly statistics

Standard assembly statistics do not tell too much about assembly quality, however they are the first thing to look at as:

- they can be used to identify an extremely problematic assembler (e.g., an assembler that has thousands of contigs more than the others)
- they can be used to understand that more parameters should be changed (the "best" assembler as an NG50 of 300bp) 

**Remember**: never (*never*, **never**, _never_) use these statistics as only source to decide which assembler/assembly is the best. ***Contiguity and Correctness are not correlated***!

```
$ cd ~/AV_Exercise/03_eval/
$ ./assembly_stats.pl --assembly ~/AV_Exercise/02_assemblies/velvet/Staphylococcus_aureus.velvet.scf.fasta --genome-length 2900000 > Staph_velvet.stats 
```

You can run this script on all others de novo assemblies (stored in the folder `~/AV_Excercise/02_assemblies`) and obtain a table similar to the one below here.

The output file reports assembly statistics twice: first in a human readable way and then in a tabular form that can be easily exported to excel. 

Assembly | GenomeSize | AssemblySize | #ctgs/scfs | MaxLength | NG50 | LG50 | NG90 | LG90 
:------- | ----------:| ------------:| ----------:| ---------:| ----:| ----:| ----:| ----:
Staphylococcus_aureus.abyss.scf.fasta | 2900000 | 3821622 | 125 | 346557 | 170210 | 6 | 78132 | 17 
Staphylococcus_aureus.allpaths.scf.fasta | 2900000 | 2880676 | 19 | 1435559 | 1091731 | 2 | 179561 | 3 
Staphylococcus_aureus.masurca.scf.fasta | 2900000 | 2872905 | 17 | 2411914 | 2411914 | 1 | 155799 | 3 
Staphylococcus_aureus.soapdenovo.scf.fasta | 2900000 | 2924135 | 175 | 518710 | 331598 | 4 | 93229 | 10 
Staphylococcus_aureus.velvet.scf.fasta | 2900000 | 2877995 | 173 | 989718 | 762333 | 2 | 142854 | 5 

#### Questions 
- What is the best assembly?
- is there any outlyer?
- What about your assembly? Does it seems close to these ones? 

### Raw data congruency

One way to evaluate an assembly is to map back reads and check if they are congruent with the contigs/scaffolds. Moreover, alignments alones are able to give us a lot of informations about the assemblies and the genome content.

We will look into three pipelines based on this approach:

- QAtools: unpublished, available on-line on git-hub.
- REAPR: publication "REAPR: a universal tool for genome assembly evaluation"
- FRCbam: publication "Re-evaluating assembly evaluation with Feature Responses Curves"

but first of all let us genearate the alignments. 

### Read Alignment

We will now see how to align both PE and MP reads against an assembly (scaffolded sequences). This step is time consuming, you can do it manually on one assembly to try the various steps. You are strongly encouraged to try this on your own assembly.

You need to work in the following directory:

```
~/AV_Exercise/04_align
```

To automatically generate all the alignments run `sh run_alignments.sh` (this will take ~30 minutes)

To softlink here pre-computed results run `sh run_soflinkAlignments.sh` (this will take ~2 minutes as PicardTools are exectued)

Let us see the various steps, we will use velvet assembly:

Create directory and collect all data:

```
$ mkdir velvet
$ cd velvet
$ ln -s ~/AV_Exercise/02_assemblies/velvet/Staphylococcus_aureus.velvet.scf.fasta .
$ ln -s ~/AV_Exercise/01_data/PE/Staphylococcus_aureus_PE_1.fastq .
$ ln -s ~/AV_Exercise/01_data/PE/Staphylococcus_aureus_PE_2.fastq .
$ ln -s ~/AV_Exercise/01_data/MP_rc/Staphylococcus_aureus_MP_rc_1.fastq .
$ ln -s ~/AV_Exercise/01_data/MP_rc/Staphylococcus_aureus_MP_rc_2.fastq .
```

Now assembly and reads are reachable from our local directory. N.B. we are using soft-links this is high reccomended in order to avoid data duplication.

Now create index:

```
$ bwa index Staphylococcus_aureus.velvet.scf.fasta
```

Now we have the index, so we can align reads with bwa. We start with the PE:

```
$ bwa mem -t 8 -M Staphylococcus_aureus.velvet.scf.fasta  Staphylococcus_aureus_PE_1.fastq Staphylococcus_aureus_PE_2.fastq > PE_on_velvet.sam
```

Now convert SAM file into BAM file (less space and more tools able to use it):

```
$ samtools view -Sb -o  PE_on_velvet.bam PE_on_velvet.sam
```

Now sort the BAM file (many tools require it sorted):

```
$ samtools sort PE_on_velvet.bam PE_on_velvet_sorted
```

Now compute Insert Size Statistics with PicardTools (extremely usefull suite of tools):

```
$ java -Xmx16g -XX:PermSize=8g -jar $PICARD_HOME/CollectInsertSizeMetrics.jar MINIMUM_PCT=0 HISTOGRAM_FILE=PE_on_velvet.pdf  INPUT=PE_on_velvet_sorted.bam  OUTPUT=PE_on_velvet_sorted.collectInseSize HISTOGRAM_WIDTH=500
```

Have a look to:

- `PE_on_velvet_sorted.collectInseSize`
- `PE_on_velvet.pdf`

What kind of information this table and plot give us? How can be used to judge the assembly?
Let us do it again for the MP:

```
$ bwa mem -t 8 -M Staphylococcus_aureus.velvet.scf.fasta  Staphylococcus_aureus_MP_rc_1.fastq Staphylococcus_aureus_MP_rc_2.fastq > MP_on_velvet.sam
$ samtools view -Sb -o  MP_on_velvet.bam MP_on_velvet.sam
$ samtools sort MP_on_velvet.bam MP_on_velvet_sorted
$ java -Xmx16g -XX:PermSize=8g -jar $PICARD_HOME/CollectInsertSizeMetrics.jar MINIMUM_PCT=0 HISTOGRAM_FILE=MP_on_velvet.pdf  INPUT=MP_on_velvet_sorted.bam  OUTPUT=MP_on_velvet_sorted.collectInseSize HISTOGRAM_WIDTH=8000
```

What kind of information this table and plot give us? How can be used to judge the assembly?

### QA tools 

QA-tools is an extremely usefull program to have a first but important understanding about the quality of the assembly. It is a tool under-development but it is important to use it.

Like before we will focus on velvet assembly. It is up to you to try the others and/or your assemblies.

We will work in this directory:

```
~/AV_Exercise/05_QAtools
```

To automatically generate all the files (with the exception of pdf files that need to be generated manully) run the command: `sh runQA_tools.sh`.

Create directory and collect the data:

```
$ mkdir velvet
$ cd velvet
$ ln -s ~/AV_Exercise/02_assemblies/velvet/Staphylococcus_aureus.velvet.scf.fasta .
$ ln -s ~/AV_Exercise/04_align/velvet/PE_on_velvet_sorted.bam .
```

We will work only with PE data as, in general, PE data is the one with high coverage and less affacted by problems (e.g, duplicated reads).

We run qaCompute (-q 0 count aligment with 0 quality, -m compute median coverage):

```
$ qaCompute -m -q 0 -i PE_on_velvet_sorted.bam  velvet.cov
```

This generates the table `velvet.cov` and some nice information is given as output. How to interpret information printed on the screen? What about the table velvet.cov? 

qaCompute does no report any information about the assembly sequences, let use the following script to add information about GC-content:

```
$ ~/AV_Exercise/00_tools/addCG.pl  Staphylococcus_aureus.velvet.scf.fasta velvet.cov > velvet.cov.gc
```

For each contig we now have the length, the coverage and the GC content. We can produce some 2D plots to inspect the result. We can use the following script to produce 2D plots:

```
$ Rscript --vanilla /proj/g2014179/assemblyValidation/tools/scripts/plotCovGC.R ASSEMBLER.cov.gc MAX_CTG_LENGTH
```

you need to play around with MAX_CTG_LENGTH value to have a nice visualisation. If you have experience with R copy the script and play around.

These are the commands used to generated the pdf files saved in your directories:

```
Rscript --vanilla /proj/g2015027/assemblyValidation/tools/scripts/plotCovGC.R abyss.cov.gc 100
Rscript --vanilla /proj/g2015027/assemblyValidation/tools/scripts/plotCovGC.R allpaths.cov.gc 200
Rscript --vanilla /proj/g2015027/assemblyValidation/tools/scripts/plotCovGC.R masurca.cov.gc 170
Rscript --vanilla /proj/g2015027/assemblyValidation/tools/scripts/plotCovGC.R soapdenovo.cov.gc 100
Rscript --vanilla /proj/g2015027/assemblyValidation/tools/scripts/plotCovGC.R velvet.cov.gc 50
```

#### Questions

- What can you say about the contigs with near-zero coverge?
- Is there a correlation between the length of the contig and the coverage? Why/why not?
- Can you start to have a feeling about quality? 

### FRCbam

[FRCbam](https://github.com/vezzi/FRC_align) takes as input one or two alignments (usually one PE and one MP library) and looks for suspicious regions where the alignments do not agree with the expected behaviour of the library. As an example places in the assembly where many reads align as singletons, or areas characterised by a too high or too low coverage.
FRCbam acknowledges the trade-off between long contigs/scaffolds and assembly errors, and will try to find the "best" assembly, using a penalty for potential errors while still acknowledging the benefit of contiguity.

We will now compute FRCurve for velvet assembly as example and we will plot the FRCurve for all the other assemblies.You need to work in the following directory:

```
~/AV_Exercise/06_FRCbam
```

run the command runFRCbam.sh to generate the data necessary to produce FRCurves

Let us see in details how it works: 

```
$ mkdir velvet
$ cd velvet
$ ln -s ~/AV_Exercise/02_assemblies/velvet/Staphylococcus_aureus.velvet.scf.fasta .
$ ln -s ~/AV_Exercise/04_align/velvet/PE_on_velvet_sorted.bam .
$ ln -s ~/AV_Exercise/04_align/velvet/MP_on_velvet_sorted.bam .
```

And now compute the FRCurve:

```
FRC --pe-sam PE_on_velvet_sorted.bam --pe-min-insert 100 --pe-max-insert 260 --mp-sam MP_on_velvet_sorted.bam --mp-min-insert 3000 --mp-max-insert 5000 --genome-size 2900000 --output velvet
```

- What information can you get from the printed output? I know it is a bit messy but there is a lot of nice information!!!
- How many features (i.e., suspicious positions) are there in the assembly?
- What are the produced files? The most important one, and the only we will use is velvet_FRC.txt

Let us now compare different assemblies usinf FRC, maybe also your. **Note**: to compare different FRCurve you MUST specify the same genome size, otherwise the curves cannot be compared.

Use the following command from folder `~/AV_Exercise/06_FRCbam/` to plot all 5 FRCurves 

```
$ mkdir FRCplot
$ cd FRCplot
$ Rscript ~/AV_Exercise/00_tools/plotFRCurves.R ../abyss/abyss_FRC.txt ../allpaths/allpaths_FRC.txt ../masurca/masurca_FRC.txt ../soapdenovo/soapdenovo_FRC.txt ../velvet/velvet_FRC.txt
```

First plot is the entire picture, in order to allow a better visualisation the second plot is a zoom.

- What do you think about ABySS? And Velvet?
- What is the best assembler accordingly to FRCbam? (i.e., the assembly that given a certain amount of features reconstructs the largest portion of the genome....)
- Who is the best between MaSuRCA and Allpaths-LG?

Play a bit around:
- plot FRCurve for a single feature (you only need to change a bit the R script.... in case you do not know nothing about R this is a great moment to start!!!!)
- rerun FRCuve using only PE data, does the scenario changes radically? 

###  REAPR

[Reapr](http://www.sanger.ac.uk/resources/software/reapr/) is a tool trying to find explicit errors in the assembly based on incongruently mapped reads. It is heavily based on too low span coverage, or reads mapping too far or too close to each other. The program will also break up contigs/scaffolds at spurious sites to form smaller (but hopefully correct) contigs.

Like always we will see how to run Reapr on velvet assembly. You nee to work in the folllwoing direcotry

```
~/AV_Exercise/07_REAPR/
```

REAPR is slow so I already copied here the results, To regenerate them run the command runREAPR.sh but it will take more than 30 minutes.

Lat us see the details:

```
$ mkdir velvet
$ cd velvet
$ ln -s ~/AV_Exercise/02_assemblies/velvet/Staphylococcus_aureus.velvet.scf.fasta .
$ ln -s ~/AV_Exercise/04_align/velvet/PE_on_velvet_sorted.bam .
$ reapr pipeline Staphylococcus_aureus.velvet.scf.fasta PE_on_velvet_sorted.bam reapr_velvet
```

look at this file: `reapr_velvet/05.summary.report.txt`

- How many assembly errors did Reapr find and break? How many warnings?
- What is the N50 before and after Reapr?
- Do the numbers coincide with FRCbam results?
- Look to the other assemblers... what do you think is the best assembler for Reapr? Why?

### CEGMA

[CEGMA](http://korflab.ucdavis.edu/datasets/cegma/) performs a HMM alignment of 248 Eukaryotic core genes to the assembly. CEGMA reports the completeness. I've allready run this analysis on the assembly to save time.

The software is pretty unstable and hopefully a new realese will soon be available. It can be used on UPPMAX (it is installed as a module) but it is pretty slow. We will only look at the reusults (anyway you will find the runCEGMA.sh script to see how to tun it, it takes more than one hour running all 5 assemblies in parallel):

```
~/AV_Exercise/08_CEGMA/ 
```

Look at this file:

```
~/AV_Exercise/08_CEGMA/velvet/Staphylococcus_aureus.velvet.scf.fasta.cegma.completeness_report
```

- How many complete (>70% aligned) core genes are there in the assembly, according to the CEGMA output? How many partial (>30% aligned)? 
